{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<script>\n",
    "    // AUTORUN ALL CELLS ON NOTEBOOK-LOAD!\n",
    "    require(\n",
    "        ['base/js/namespace', 'jquery'], \n",
    "        function(jupyter, $) {\n",
    "            $(jupyter.events).on(\"kernel_ready.Kernel\", function () {\n",
    "                console.log(\"Auto-running all cells-below...\");\n",
    "                jupyter.actions.call('jupyter-notebook:run-all-cells-below');\n",
    "                jupyter.actions.call('jupyter-notebook:save-notebook');\n",
    "            });\n",
    "        }\n",
    "    );\n",
    "</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIP INSTALLS\n",
    "\n",
    "# for data handling:\n",
    "#! pip install pandas\n",
    "\n",
    "# for reading Duration entries (WIP, unused yet):\n",
    "#! pip install text2digits\n",
    "\n",
    "# for imputing lat/long:\n",
    "#! pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from numpy import select\n",
    "from functools import partial\n",
    "from text2digits import text2digits\n",
    "from datetime import datetime,timedelta\n",
    "from geopy.geocoders import Nominatim#,OpenCage\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "\n",
    "# initialise things:\n",
    "ua = \\\n",
    "\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36\"\n",
    "k = \\\n",
    "\"cc6fe73ae2a64a7c937b8f8553dce0a1\"\n",
    "\n",
    "Geogetter = partial(Nominatim(user_agent=ua).geocode , language='en')\n",
    "#Geogetter = partial(OpenCage(k).geocode , language='en')\n",
    "Geoget = RateLimiter(Geogetter , min_delay_seconds=1)\n",
    "\n",
    "Reader = text2digits.Text2Digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "\n",
    "statesUSA = {\n",
    "    'AL' : 'Alabama',\n",
    "    'AK' : 'Alaska',\n",
    "    'AS' : 'American Samoa',\n",
    "    'AZ' : 'Arizona',\n",
    "    'AR' : 'Arkansas',\n",
    "    'CA' : 'California',\n",
    "    'CO' : 'Colorado',\n",
    "    'CT' : 'Connecticut',\n",
    "    'DE' : 'Delaware',\n",
    "    'DC' : 'District of Columbia',\n",
    "    'FL' : 'Florida',\n",
    "    'GA' : 'Georgia',\n",
    "    'GU' : 'Guam',\n",
    "    'HI' : 'Hawaii',\n",
    "    'ID' : 'Idaho',\n",
    "    'IL' : 'Illinois',\n",
    "    'IN' : 'Indiana',\n",
    "    'IA' : 'Iowa',\n",
    "    'KS' : 'Kansas',\n",
    "    'KY' : 'Kentucky',\n",
    "    'LA' : 'Louisiana',\n",
    "    'ME' : 'Maine',\n",
    "    'MD' : 'Maryland',\n",
    "    'MA' : 'Massachusetts',\n",
    "    'MI' : 'Michigan',\n",
    "    'MN' : 'Minnesota',\n",
    "    'MS' : 'Mississippi',\n",
    "    'MO' : 'Missouri',\n",
    "    'MT' : 'Montana',\n",
    "    'NE' : 'Nebraska',\n",
    "    'NV' : 'Nevada',\n",
    "    'NH' : 'New Hampshire',\n",
    "    'NJ' : 'New Jersey',\n",
    "    'NM' : 'New Mexico',\n",
    "    'NY' : 'New York',\n",
    "    'NC' : 'North Carolina',\n",
    "    'ND' : 'North Dakota',\n",
    "    'OH' : 'Ohio',\n",
    "    'OK' : 'Oklahoma',\n",
    "    'OR' : 'Oregon',\n",
    "    'PA' : 'Pennsylvania',\n",
    "    'PR' : 'Puerto Rico',\n",
    "    'RI' : 'Rhode Island',\n",
    "    'SC' : 'South Carolina',\n",
    "    'SD' : 'South Dakota',\n",
    "    'TN' : 'Tennessee',\n",
    "    'TX' : 'Texas',\n",
    "    'UT' : 'Utah',\n",
    "    'VT' : 'Vermont',\n",
    "    'VA' : 'Virginia',\n",
    "    'WA' : 'Washington',\n",
    "    'WV' : 'West Virginia',\n",
    "    'WI' : 'Wisconsin',\n",
    "    'WY' : 'Wyoming'\n",
    "}\n",
    "\n",
    "statesCND = {\n",
    "  'AB' : 'Alberta',\n",
    "  'BC' : 'British Columbia',\n",
    "  'MB' : 'Manitoba',\n",
    "  'NB' : 'New Brunswick',\n",
    "  'NL' : 'Newfoundland and Labrador',\n",
    "  'NS' : 'Nova Scotia',\n",
    "  'NT' : 'Northwest Territories',\n",
    "  'NU' : 'Nunavut',\n",
    "  'ON' : 'Ontario',\n",
    "  'PE' : 'Prince Edward Island',\n",
    "  'QC' : 'Quebec',\n",
    "  'SK' : 'Saskatchewan',\n",
    "  'YT' : 'Yukon'\n",
    "}\n",
    "\n",
    "allStates = {**statesUSA, **statesCND}\n",
    "\n",
    "shapeMapping = {\"light\"     : \"Flash\",\n",
    "                \"circle\"    : \"Orb\",\n",
    "                \"teardrop\"  : \"Teardrop\",\n",
    "                \"cigar\"     : \"Cylinder\",\n",
    "                \"disk\"      : \"Disc\",\n",
    "                \"unknown\"   : \"Indiscernible\",\n",
    "                \"oval\"      : \"Egg\",\n",
    "                \"other\"     : \"Indiscernible\",\n",
    "                \"sphere\"    : \"Orb\",\n",
    "                \"changing\"  : \"Metamorphing\",\n",
    "                \"formation\" : \"Swarm\",\n",
    "                \"flash\"     : \"Flash\",\n",
    "                \"chevron\"   : \"Chevron\",\n",
    "                \"0\"         : \"Egg\",\n",
    "                \"triangle\"  : \"Triangle\",\n",
    "                \"fireball\"  : \"Flash\",\n",
    "                \"cross\"     : \"Cross\",\n",
    "                \"rectangle\" : \"Rectangle\",\n",
    "                \"diamond\"   : \"Diamond\",\n",
    "                \"cylinder\"  : \"Cylinder\",\n",
    "                \"egg\"       : \"Egg\",\n",
    "                \"cone\"      : \"Cone\"}\n",
    "\n",
    "regexes = {\n",
    "    1      : r'(?<![A-z])se?c?o?n?d?s?',\n",
    "    2      : r'(?<![A-z])m(?:in)?(?:ute)?s?',\n",
    "    3      : r'(?<![A-z])ho?u?r?s?',\n",
    "    'time' : r'\\d{1,2}\\s?[:.;]\\s?(?:\\d{1,2}\\s?[:.;]\\s?)?\\d\\d(?=\\D|\\b)',\n",
    "    'vals' : r'\\.?\\d+ ?[.,]? ?\\d*'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRIVATE\n",
    "\n",
    "def _findnth(string , substring , n):\n",
    "    \"\"\" Find n-th occurance \"\"\"\n",
    "    # https://stackoverflow.com/a/13094326/12955879\n",
    "    if n == 1:\n",
    "        return string.find(substring)\n",
    "    else:\n",
    "        return string.find( substring , _findnth(string,substring,n-1)+1 )\n",
    "\n",
    "    \n",
    "def _sieve(dictionary):\n",
    "    \"\"\" Return key(s) of non-empty dictionary entry \"\"\"\n",
    "    return [k for k,v in dictionary.items() if v]\n",
    "\n",
    "\n",
    "def _datetimeToSeconds(datetimeObj):\n",
    "    return _timediff([datetime(1900,1,1) , datetimeObj]).total_seconds()\n",
    "\n",
    "\n",
    "def _timediff(window):\n",
    "    \"\"\" Difference of two datetime objects, accounting for spans across midnight \"\"\"\n",
    "    # can be done as (end + 24 - start) % 24 but this is simpler/more readable.\n",
    "    if window[0] <= window[1]:\n",
    "        return window[1] - window[0]\n",
    "    else:\n",
    "        return window[1] + timedelta(days=1) - window[0]\n",
    "\n",
    "\n",
    "def _findSingleton(timescales):\n",
    "    return {\n",
    "        i+1 : 60**i for i in range(3)\n",
    "    }[_sieve(timescales)]\n",
    "\n",
    "\n",
    "def _findTime(times , scales=dict()):\n",
    "    sievedScales = _sieve(scales)\n",
    "    \n",
    "    disqualifiers = [\n",
    "        not sievedScales and len(times) == 1,\n",
    "        len(times) > 2,\n",
    "        len(sievedScales) > len(times)\n",
    "    ]\n",
    "    if any(disqualifiers):  return None\n",
    "    \n",
    "    print(sievedScales)\n",
    "    window = list()\n",
    "    if not sievedScales:\n",
    "        for time,pos in times:\n",
    "            form = '%H:%M:%S' if time.count(':')==1 else '%M:%S'\n",
    "            window.append( datetime.strptime(time,form) )\n",
    "    elif len(sievedScales) == 1:\n",
    "        form = {\n",
    "            3 : '%H:%M:%S',\n",
    "            2 : '%M:%S:%f',\n",
    "            1 : '%S:%f'                 # assuming we don't get any '00:00:05 seconds'? Damn... I'm not gonna bother.\n",
    "        }[int( *sievedScales )]\n",
    "        for time,pos in times:\n",
    "            if (time.count(':') == 2) and (1 in sievedScales):\n",
    "                pass\n",
    "            chop = slice(5) if time.count(':')==1 else slice(8)\n",
    "            window.append( datetime.strptime(time,form[chop]) )\n",
    "    elif len(sievedScales) == 2:\n",
    "        # WIP\n",
    "        pass\n",
    "\n",
    "    if len(window) == 2:\n",
    "        span = _timediff(window).total_seconds()\n",
    "        if any(map( lambda time:time[0].count(':')==1 , times )):\n",
    "            window[-1] = window[0] + timedelta(seconds=span/2)          # average if any 2-parters\n",
    "        else:\n",
    "            return span                                                 # difference for 3-parters only\n",
    "    \n",
    "    return _datetimeToSeconds(window[-1])\n",
    "    \n",
    "def _extractDurationData(text):\n",
    "    raw = text\n",
    "    \n",
    "    # catches 2- or 3-part clock formats:\n",
    "    times = list()\n",
    "    for time in re.findall(regexes['time'] , text):\n",
    "        text = text.replace(time , ' ' , 1)\n",
    "        clean = time.replace(' ','').replace(';',':').replace('.',':')\n",
    "        clean = ':'.join( part.zfill(2) for part in clean.split(':') )\n",
    "        times.append( (clean , raw.find(time)) )\n",
    "        \n",
    "    # catches leftover numerics:\n",
    "    values = list()\n",
    "    for value in re.findall(regexes['vals'] , text):\n",
    "        text = text.replace(value , ' ' , 1)\n",
    "        clean = value.replace(' ','').replace(',','.')\n",
    "        values.append( (clean , raw.find(value)) )\n",
    "    \n",
    "    # catches leftover timescale words:\n",
    "    scales = {\n",
    "        1 : list(),\n",
    "        2 : list(),\n",
    "        3 : list()\n",
    "    }\n",
    "    for sc in scales:\n",
    "        for i,match in enumerate( re.findall(regexes[sc],text) ):\n",
    "            text = text.replace(match , ' ' , 1)\n",
    "            scales[sc].append( _findnth(raw,match,i+1) )\n",
    "    \n",
    "    return times,values,scales\n",
    "\n",
    "    \n",
    "def _countryName(geo):\n",
    "    name = geo.address.split(', ')[-1]\n",
    "    # Nomatim returns 'United States of America', so fix that:\n",
    "    return name if 'United States' not in name else 'United States'\n",
    "\n",
    "\n",
    "def _assignGeo(m,city,abbrev):\n",
    "    \"\"\" Acts as a switch/case for imputing geographical data. \"\"\"\n",
    "    # columns are, in order: Country - Latitude - Longitude\n",
    "    # 3 gaps - all 3 need imputation\n",
    "    # 2 gaps - lat/long need imputation (they always come together)\n",
    "    # 1 gap  - only Country needs imputation\n",
    "    address = city\n",
    "    if abbrev in allStates.keys():\n",
    "        address += \", \" + allStates[abbrev]\n",
    "    \n",
    "    geo = Geoget(address)\n",
    "    \n",
    "    if not geo:\n",
    "        print(city)\n",
    "        return (None,None,None)\n",
    "    else:\n",
    "        return {\n",
    "            3 : ( _countryName(geo) , geo.latitude , geo.longitude ),\n",
    "            2 : ( None , geo.latitude , geo.longitude ),\n",
    "            1 : ( _countryName(geo) , None , None )\n",
    "        }[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TEST CELL FOR READING DURATION DATA\n",
    "\n",
    "tests = list()\n",
    "tests.append('05:00hrs to 06:00hrs')\n",
    "tests.append('0.05m')\n",
    "tests.append('30:00 to 40:00')\n",
    "tests.append('15 sec')\n",
    "tests.append('2h to 3h')\n",
    "tests.append('1 min 30s')\n",
    "tests.append('06:30 to 08:00hrs')\n",
    "tests.append('1and 1/2 minutes')\n",
    "tests.append('15s to 1 min')\n",
    "tests.append('1/2 hr')\n",
    "tests.append('For 5-10 Min')\n",
    "\n",
    "test = pd.Series(tests)\n",
    "\n",
    "test = cleanDuration(test)\n",
    "\n",
    "for text in test:\n",
    "    # COMBINATIONS:\n",
    "    #  singleton:\n",
    "    #   only 3-part time       - timediff from Time field (ignore for now)\n",
    "    #   only 2-part time       - DURATION (as mm:ss) / ignore?\n",
    "    #   only value             - ignore\n",
    "    #   only scale             - assume 1 ('hour' is 01:00:00)\n",
    "    #  pair:\n",
    "    #   2x 3-times             - average (assume leftmost is hours)\n",
    "    #   2x 2-times             - average (assume leftmost is minutes)\n",
    "    #   value-scale pair       - DURATION\n",
    "    #   2/3-time & scale       - DURATION with leftmost part as scale (e.g. '6:45hrs' is 06:45:00, '02:30m' is 00:02:30)\n",
    "    #  triplet:\n",
    "    #   value-value and scale  - average, with scale applying to both (e.g. '3-5 mins' is 00:04:00)\n",
    "    #   value-scale-value      - fuse with scale as first value (e.g. '1m30' is 00:01:30)\n",
    "    #   2x 2/3-times & scale   - timediff, with scale applying to both ('6:15-6:45hrs' is 00:30:00)\n",
    "    #  quadruplet:\n",
    "    #   2x value-scale pairs decreasing   - fuse (e.g. '2 mins 30 sec' is 00:02:30)\n",
    "    #   2x value-scale pairs monotone     - average (e.g. '30 sec - 2 mins' is 00:01:15, '4 min - 5 min' is 00:04:30)\n",
    "    #   2x 2/3-time and 2x scales         - timediff with scale as leftmost part (e.g. '30:00m - 1:00hr')\n",
    "    #   value-scale and time-scale        - average in pairs (e.g. '30s to 01:00m' is 00:00:45)\n",
    "    dur = None\n",
    "    \n",
    "    times,values,scales = _extractDurationData(text)\n",
    "    \n",
    "    print(\"text:\",text)\n",
    "    print(\"times:\",times)\n",
    "    print(\"values:\",values)\n",
    "    print(\"scales:\",scales)\n",
    "\n",
    "    scaleCounts = {\n",
    "        3 : len(scales[3]),           # hour\n",
    "        2 : len(scales[2]),           # minute\n",
    "        1 : len(scales[1])            # second\n",
    "    }\n",
    "    componentCounts = {\n",
    "        'times'  : len(times),\n",
    "        'values' : len(values),\n",
    "        'scale'  : sum(scaleCounts.values())\n",
    "    }\n",
    "    totalCount = sum(componentCounts.values())\n",
    "    print(\"components:\",componentCounts)\n",
    "    print(\"timescales:\",scaleCounts)\n",
    "    \n",
    "    # S       - assume 1                 'hour'\n",
    "    # T S     - scale is leftmost time   '05:00 mins'\n",
    "    # V S     - scale is value           '5hrs'\n",
    "    # T T\n",
    "    #   2T 2T  - mean with MM:SS            '03:00 to 03:30'\n",
    "    #   2T 3T  - ignore\n",
    "    #   3T 2T  - ignore\n",
    "    #   3T 3T  - diff with HH:MM:SS         '22:30:00 to 01:00:00'\n",
    "    # T T S   - scale is leftmost time   '03:00 to 03:45 mins'\n",
    "    #   config irrelevant\n",
    "    # V V S   - scale is both values     '3-5 hrs'\n",
    "    # V S V   - scale is first value     '1 min 30'\n",
    "    # rest are combinations of these.\n",
    "    #\n",
    "    if totalCount == 1 and componentCounts['scale'] == 1:\n",
    "        dur = _findSingleton(scales)\n",
    "    elif totalCount == 2:\n",
    "        if componentCounts['times'] == 2:\n",
    "            dur = _findTime(times)\n",
    "        elif componentCounts['times'] == 1 and componentCounts['scale'] == 1:\n",
    "            dur = _findTime(times,scales)\n",
    "        elif componentCounts['values'] == 1 and componentCounts['scale'] == 1:\n",
    "            pass\n",
    "        \n",
    "    print(\"duration (s):\",dur)\n",
    "    print()\n",
    "    #return dur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBLIC\n",
    "\n",
    "def cleanDuration(serie):\n",
    "    # preserve floats and homogenises case:\n",
    "    serie = serie.astype(str)\n",
    "    serie = serie.str.lower()\n",
    "    \n",
    "    # common fractions made decimal, so entries like '5/6 hours' can be read as 05:30:00\n",
    "    serie = serie.str.replace(r'(?:and|&)\\s?(?:a)?' , ' ')\n",
    "    serie = serie.str.replace(r'half' , '1/2')\n",
    "    serie = serie.str.replace(r'(?<!\\d)\\s*1[\\/\\\\]4(?!\\d)' , '.25')\n",
    "    serie = serie.str.replace(r'(?<!\\d)\\s*1[\\/\\\\]3(?!\\d)' , '.33')\n",
    "    serie = serie.str.replace(r'(?<!\\d)\\s*1[\\/\\\\]2(?!\\d)' , '.5')\n",
    "    \n",
    "    return serie\n",
    "\n",
    "\n",
    "def readDuration(text):\n",
    "    \"\"\" Using regex procedures to remedy the godless decision to enter duration as freetext. \"\"\"\n",
    "    # Each Duration entry is made of any number of COMPONENTS:\n",
    "    # TIMES\n",
    "    #  2-part:\n",
    "    #   00:45      (could be hh:mm or mm:ss)\n",
    "    #   6:45       (could be h:mm or m:ss)\n",
    "    #  3-part:\n",
    "    #   08:15:25   (hh:mm:ss)\n",
    "    # \n",
    "    # VALUES\n",
    "    #  5           (leave as-is)\n",
    "    #  3.3         (treat as literal, so 3:30)\n",
    "    #  3.5         (special case, assume 3:30 too)\n",
    "    #  >= 3.6      (treat as fraction, so >= 3:36)\n",
    "    # \n",
    "    # SCALES\n",
    "    #  s sec secs second seconds\n",
    "    #  m min mins minute minutes\n",
    "    #  h hr hrs hour hours\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # COMBINATIONS:\n",
    "    #  singleton:\n",
    "    #   only 3-part time       - timediff from Time field (ignore for now)\n",
    "    #   only 2-part time       - DURATION (as mm:ss) / ignore?\n",
    "    #   only value             - ignore\n",
    "    #   only scale             - assume 1 ('hour' is 01:00:00)\n",
    "    #  pair:\n",
    "    #   2x 3-times             - average (assume leftmost is hours)\n",
    "    #   2x 2-times             - average (assume leftmost is minutes)\n",
    "    #   two values/scales      - ignore\n",
    "    #   value-scale pair       - DURATION\n",
    "    #   2/3-time & scale       - DURATION with leftmost part as scale (e.g. '6:45hrs' is 06:45:00, '02:30m' is 00:02:30)\n",
    "    #  triplet:\n",
    "    #   value-value and scale  - average, with scale applying to both (e.g. '3-5 mins' is 00:04:00)\n",
    "    #   value-scale-value      - fuse with scale as first value (e.g. '1m30' is 00:01:30)\n",
    "    #   2x 2/3-times & scale   - timediff, with scale applying to both ('6:15-6:45hrs' is 00:30:00)\n",
    "    #  quadruplet:\n",
    "    #   2x value-scale pairs decreasing   - fuse (e.g. '2 mins 30 sec' is 00:02:30)\n",
    "    #   2x value-scale pairs monotone     - average (e.g. '30 sec - 2 mins' is 00:01:15, '4 min - 5 min' is 00:04:30)\n",
    "    #   2x 2/3-time and 2x scales         - timediff with scale as leftmost part (e.g. '30:00m - 1:00hr')\n",
    "    #   value-scale and time-scale        - average in pairs (e.g. '30s to 01:00m' is 00:00:45)\n",
    "    dur = None\n",
    "    \n",
    "    times,values,scales = _extractDurationData(text)\n",
    "    \n",
    "    scaleCounts = {\n",
    "        3 : len(scales[3]),           # hour\n",
    "        2 : len(scales[2]),           # minute\n",
    "        1 : len(scales[1])            # second\n",
    "    }\n",
    "    componentCounts = {\n",
    "        'times'  : len(times),\n",
    "        'values' : len(values),\n",
    "        'scale'  : sum(scaleCounts.values())\n",
    "    }\n",
    "    totalCount = sum(componentCounts.values())\n",
    "    \n",
    "    # S       - assume 1                 'hour'\n",
    "    # T S     - scale is leftmost time   '05:00 mins'\n",
    "    # V S     - scale is value           '5hrs'\n",
    "    # T T\n",
    "    #   2T 2T  - mean with MM:SS            '03:00 to 03:30'\n",
    "    #   2T 3T  - ignore\n",
    "    #   3T 2T  - ignore\n",
    "    #   3T 3T  - diff with HH:MM:SS         '22:30:00 to 01:00:00'\n",
    "    # T T S   - scale is leftmost time   '03:00 to 03:45 mins'\n",
    "    #   config irrelevant\n",
    "    # V V S   - scale is both values     '3-5 hrs'\n",
    "    # V S V   - scale is first value     '1 min 30'\n",
    "    # rest are combinations of these (?)\n",
    "    if totalCount == 1 and componentCounts['scale'] == 1:\n",
    "        dur = _findSingleton(scales)\n",
    "    elif totalCount == 2:\n",
    "        if componentCounts['times'] == 2:\n",
    "            dur = _findTime(times)\n",
    "        elif componentCounts['times'] == 1 and componentCounts['scale'] == 1:\n",
    "            dur = _findTime(times,scales)\n",
    "        elif componentCounts['values'] == 1 and componentCounts['scale'] == 1:\n",
    "            # WIP\n",
    "            pass\n",
    "    \n",
    "    return dur\n",
    "\n",
    "    # limitations:\n",
    "    # yet to figure out how to handle recurring sightings. Anything over the scale of hours is ignored currently.\n",
    "    # other languages not considered \"1 hora\" gets nullified\n",
    "    # number-words are ignored (for now) \"twenty minutes\" or \"an hour\"\n",
    "    # ~1,000 entries use the Duration field as an end time (relative to Time field), so could take difference?\n",
    "    # order between components is not considered â€” e.g. '5 minutes 15' is computed same as '5 to 15 minutes'; 00:10:00\n",
    "\n",
    "\n",
    "def imputeLocal(states):\n",
    "    \"\"\" More accurate country labelling. \"\"\"\n",
    "    # For 554 \"U.S.\" entries, a state isn't even given. A lot of these are foreign cities\n",
    "    #  without even an American town named after them, meaning the label is plainly incorrect.\n",
    "    #  A few are also Puerto Rican towns, so I included it in the set of (American) States.\n",
    "    conds = [\n",
    "        states.isnull(),\n",
    "        states.isin( set(statesUSA.keys()) ),\n",
    "        states.isin( set(statesCND.keys()) )\n",
    "    ]\n",
    "    choices = [None , 'United States' , 'Canada']        \n",
    "    return select(conds , choices)\n",
    "\n",
    "\n",
    "def imputeWeb(missinglist , cities , states):\n",
    "    # inline is faster apparently, and I need speed.\n",
    "    overlay = [ _assignGeo(m,city,state) if m>0 else (None,None,None) for m,city,state in zip(missinglist,cities,states) ]\n",
    "    return pd.DataFrame( overlay , columns=['Country','Latitude','Longitude'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for i in range(1):\n",
    "        file = 'nuforc_raw'\n",
    "        DF = pd.read_excel(\n",
    "            file+'.xlsx',\n",
    "            parse_dates=[['Date','Time']]\n",
    "        )\n",
    "        \n",
    "        # Time:\n",
    "        DF.rename(\n",
    "            columns = {'Date_Time':'Datetime'},\n",
    "            inplace = True\n",
    "        )\n",
    "        \n",
    "        # Shape:\n",
    "        DF['Shape'] = DF.Shape.map(shapeMapping).astype('category')\n",
    "\n",
    "        # Country (impute local, only U.S. or Canada):\n",
    "        countries_old = DF.Country.copy()\n",
    "        DF['Country'] = imputeLocal(DF.State)\n",
    "\n",
    "        # Country, Latitude, Longitude (impute from web):\n",
    "        if False:\n",
    "            toImpute = ['Country','Latitude','Longitude']\n",
    "            missingnos = DF[toImpute].isnull().sum(axis=1).to_list()\n",
    "            DF[toImpute] = DF[toImpute].combine_first(\n",
    "                imputeWeb(missingnos , DF.City.values , DF.State.values)\n",
    "            )\n",
    "        \n",
    "        # reinstate original country if no better guess\n",
    "        DF['Country'] = DF.Country.combine_first(countries_old)\n",
    "\n",
    "        # Duration:\n",
    "        if False:\n",
    "            DF['Duration'] = cleanDuration(DF.Duration)\n",
    "            DF['Duration'] = DF.Duration.map(\n",
    "                readDuration,\n",
    "                na_action='ignore'\n",
    "            )\n",
    "            DF = DF.rename({'Duration':'Duration (s)'} , axis=1)\n",
    "\n",
    "        print(i, \"\\n\" , DF)\n",
    "        DF.to_excel( file+'_final.xlsx' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
